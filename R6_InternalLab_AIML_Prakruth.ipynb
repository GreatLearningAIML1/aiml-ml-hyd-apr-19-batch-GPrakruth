{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_InternalLab_AIML_Prakruth.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YJRBuqXhOB7_",
        "_4bLlfQmLO6V",
        "3GlbpGYELO6Z",
        "70LKIEm3LO6b",
        "10tHGSsGLO6h"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sb7Epo0VOB58"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fHpCNRv1OB5-",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwC-cVT2L9nc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "324b7f16-84e1-4e6c-ce73-7951efce0e15"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeYBYIOpLO4S",
        "colab_type": "text"
      },
      "source": [
        "### Enable Eager Execution if you are using tensorflow 1.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnSsH8sNOB6F",
        "colab": {}
      },
      "source": [
        "##Enable Eager Execution\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DxJDmJqqOB6K"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FhllFLyKOB6N",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B4yQKMiJOB6R",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('./prices.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fgkX6SEqOB6W"
      },
      "source": [
        "### Check all columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7K8pWsNQOB6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "605c7db5-44f2-4a69-83d6-11f3197414b6"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['date', 'symbol', 'open', 'close', 'low', 'high', 'volume'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7dU6X7MpOB6c"
      },
      "source": [
        "### Drop columns `date` and  `symbol`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lh_6spSKOB6e",
        "colab": {}
      },
      "source": [
        "data = data.drop(columns = ['date','symbol'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xlwbUgTwOB6i",
        "outputId": "3da7f028-835d-4fd6-a8a0-030150ad712b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DBv3WWYOB6q"
      },
      "source": [
        "### Consider only first 1000 rows in the dataset for building feature set and target set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_hG9rGBOB6s",
        "colab": {}
      },
      "source": [
        "data_new = data.head(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMOkGKc1LO5C",
        "colab_type": "text"
      },
      "source": [
        "### Convert Float64 to Float32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vn1H1uwLO5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_new = data_new.astype('float32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYpIhPoPQRLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data_new.drop(columns='close')\n",
        "Y = data_new.close"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBwwz33mW4bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlNPyphoQPyR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M3UaApqYOB6x"
      },
      "source": [
        "### Divide the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtaHe7gxPyPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EkKAy7fOB6y",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test= train_test_split(X,Y,test_size =0.2,random_state = 123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg8tMvKVZKyj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90740087-b56e-42c8-8653-ea491d7aab52"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-N2U6kbLO5N",
        "colab_type": "text"
      },
      "source": [
        "### Normalize Train and Test Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElpbKjAOLO5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import Normalizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAHL_XwsUhRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "15dbbca6-343a-4207-a2a0-4e89579337b2"
      },
      "source": [
        "train_x"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.2876651e-05, 2.2689075e-05, 2.3034214e-05, 1.0000000e+00],\n",
              "       [4.7349291e-05, 4.6249999e-05, 4.7349291e-05, 1.0000000e+00],\n",
              "       [7.6225746e-07, 7.5391358e-07, 7.7544627e-07, 1.0000000e+00],\n",
              "       ...,\n",
              "       [2.3780945e-05, 2.3505878e-05, 2.4289406e-05, 1.0000000e+00],\n",
              "       [1.7972880e-05, 1.7853366e-05, 1.8119972e-05, 1.0000000e+00],\n",
              "       [3.1503550e-05, 3.1358217e-05, 3.1951997e-05, 1.0000000e+00]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lgoDlbLZQPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fc1eedc-1672-4b9a-ec4d-3db8139ed490"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512     30.350000\n",
              "685     52.730000\n",
              "997     28.770000\n",
              "927     14.940000\n",
              "376     26.790001\n",
              "788     23.209997\n",
              "91     123.910004\n",
              "170    128.550003\n",
              "584     25.000000\n",
              "90     122.970001\n",
              "75     123.790001\n",
              "747     69.430000\n",
              "969     37.380001\n",
              "368     56.060001\n",
              "893     98.089996\n",
              "529     27.430000\n",
              "870     25.680000\n",
              "97     126.620003\n",
              "897     66.949997\n",
              "272     22.330000\n",
              "881     10.960000\n",
              "936     37.169998\n",
              "270     29.889999\n",
              "708     18.719999\n",
              "483     23.340000\n",
              "610     27.120001\n",
              "431     59.230000\n",
              "754     37.700001\n",
              "729     66.019997\n",
              "279     38.599998\n",
              "          ...    \n",
              "208    125.900002\n",
              "608     24.629999\n",
              "420     49.430000\n",
              "253     40.380001\n",
              "846     47.560001\n",
              "339     30.500000\n",
              "409     48.880001\n",
              "111    124.959999\n",
              "224    124.820000\n",
              "942     30.350000\n",
              "544     30.950001\n",
              "73     122.309998\n",
              "47     120.629997\n",
              "638     20.440001\n",
              "113    124.839996\n",
              "96     125.949997\n",
              "737     29.330000\n",
              "214    118.790001\n",
              "569     45.579998\n",
              "123    124.309998\n",
              "106    129.250000\n",
              "595     60.599998\n",
              "17     114.470001\n",
              "742     36.779999\n",
              "98     126.459999\n",
              "988     48.150002\n",
              "322     32.529999\n",
              "382     29.100000\n",
              "365     38.959999\n",
              "510     76.849998\n",
              "Name: close, Length: 800, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tHP9itBPAyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Normalizer()\n",
        "train_x = transformer.fit_transform(X_train)\n",
        "test_x = transformer.fit_transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v6vE4eYCOB62"
      },
      "source": [
        "## Building the graph in tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "297_qja4OB7A"
      },
      "source": [
        "2.Define Weights and Bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L205qPeQOB7B",
        "colab": {}
      },
      "source": [
        "#We are initializing weights and Bias with Zero\n",
        "w = tf.zeros(shape=(4,1))\n",
        "b = tf.zeros(shape=(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HgtWA-UIOB7F"
      },
      "source": [
        "3.Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JveGlx25OB7H",
        "colab": {}
      },
      "source": [
        "def prediction(x, w, b):\n",
        "    \n",
        "    xw_matmul = tf.matmul(x, w)\n",
        "    y = tf.add(xw_matmul, b)\n",
        "    \n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TL1hIwf_OB7M"
      },
      "source": [
        "4.Loss (Cost) Function [Mean square error]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8VSWPiGXOB7P",
        "colab": {}
      },
      "source": [
        "def loss(y_actual, y_predicted):\n",
        "    \n",
        "    diff = y_actual - y_predicted\n",
        "    sqr = tf.square(diff)\n",
        "    avg = tf.reduce_mean(sqr)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jzG85FUlOB7U"
      },
      "source": [
        "5.GradientDescent Optimizer to minimize Loss [GradientDescentOptimizer]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cj802w-3OB7X",
        "colab": {}
      },
      "source": [
        "def train(x, y_actual, w, b, learning_rate=0.01):\n",
        "    \n",
        "    #Record mathematical operations on 'tape' to calculate loss\n",
        "    with tf.GradientTape() as t:\n",
        "        \n",
        "        t.watch([w,b])\n",
        "        \n",
        "        current_prediction = prediction(x, w, b)\n",
        "        current_loss = loss(y_actual, current_prediction)\n",
        "    \n",
        "    #Calculate Gradients for Loss with respect to Weights and Bias\n",
        "    dw, db = t.gradient(current_loss,[w, b])\n",
        "    \n",
        "    #Update Weights and Bias\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "    \n",
        "    return w, b "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ4srRkoZa5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSypb_u8OB7e"
      },
      "source": [
        "## Execute the Graph for 100 epochs and observe the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DVvgj7eQOB7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88846141-03bb-4a5c-a1f5-6547c56bafe7"
      },
      "source": [
        "for i in range(100):\n",
        "    \n",
        "    w, b = train(train_x, np.array(y_train), w, b)\n",
        "    print('Current Loss on iteration', i, loss(np.array(y_train), prediction(train_x, w, b)).numpy())"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Loss on iteration 0 7968.088\n",
            "Current Loss on iteration 1 7639.777\n",
            "Current Loss on iteration 2 7337.2095\n",
            "Current Loss on iteration 3 7058.362\n",
            "Current Loss on iteration 4 6801.3794\n",
            "Current Loss on iteration 5 6564.5425\n",
            "Current Loss on iteration 6 6346.2725\n",
            "Current Loss on iteration 7 6145.112\n",
            "Current Loss on iteration 8 5959.727\n",
            "Current Loss on iteration 9 5788.877\n",
            "Current Loss on iteration 10 5631.4185\n",
            "Current Loss on iteration 11 5486.3057\n",
            "Current Loss on iteration 12 5352.5703\n",
            "Current Loss on iteration 13 5229.322\n",
            "Current Loss on iteration 14 5115.733\n",
            "Current Loss on iteration 15 5011.049\n",
            "Current Loss on iteration 16 4914.574\n",
            "Current Loss on iteration 17 4825.6616\n",
            "Current Loss on iteration 18 4743.72\n",
            "Current Loss on iteration 19 4668.2026\n",
            "Current Loss on iteration 20 4598.608\n",
            "Current Loss on iteration 21 4534.4663\n",
            "Current Loss on iteration 22 4475.356\n",
            "Current Loss on iteration 23 4420.877\n",
            "Current Loss on iteration 24 4370.669\n",
            "Current Loss on iteration 25 4324.4014\n",
            "Current Loss on iteration 26 4281.76\n",
            "Current Loss on iteration 27 4242.4624\n",
            "Current Loss on iteration 28 4206.2407\n",
            "Current Loss on iteration 29 4172.8633\n",
            "Current Loss on iteration 30 4142.1016\n",
            "Current Loss on iteration 31 4113.751\n",
            "Current Loss on iteration 32 4087.624\n",
            "Current Loss on iteration 33 4063.5457\n",
            "Current Loss on iteration 34 4041.3533\n",
            "Current Loss on iteration 35 4020.9023\n",
            "Current Loss on iteration 36 4002.0537\n",
            "Current Loss on iteration 37 3984.6848\n",
            "Current Loss on iteration 38 3968.6753\n",
            "Current Loss on iteration 39 3953.9216\n",
            "Current Loss on iteration 40 3940.3247\n",
            "Current Loss on iteration 41 3927.7944\n",
            "Current Loss on iteration 42 3916.2456\n",
            "Current Loss on iteration 43 3905.6023\n",
            "Current Loss on iteration 44 3895.7937\n",
            "Current Loss on iteration 45 3886.7527\n",
            "Current Loss on iteration 46 3878.4224\n",
            "Current Loss on iteration 47 3870.7456\n",
            "Current Loss on iteration 48 3863.6704\n",
            "Current Loss on iteration 49 3857.1497\n",
            "Current Loss on iteration 50 3851.14\n",
            "Current Loss on iteration 51 3845.6\n",
            "Current Loss on iteration 52 3840.496\n",
            "Current Loss on iteration 53 3835.7913\n",
            "Current Loss on iteration 54 3831.456\n",
            "Current Loss on iteration 55 3827.46\n",
            "Current Loss on iteration 56 3823.7783\n",
            "Current Loss on iteration 57 3820.384\n",
            "Current Loss on iteration 58 3817.2568\n",
            "Current Loss on iteration 59 3814.3752\n",
            "Current Loss on iteration 60 3811.7175\n",
            "Current Loss on iteration 61 3809.2705\n",
            "Current Loss on iteration 62 3807.0144\n",
            "Current Loss on iteration 63 3804.936\n",
            "Current Loss on iteration 64 3803.0193\n",
            "Current Loss on iteration 65 3801.252\n",
            "Current Loss on iteration 66 3799.6255\n",
            "Current Loss on iteration 67 3798.1265\n",
            "Current Loss on iteration 68 3796.7424\n",
            "Current Loss on iteration 69 3795.4705\n",
            "Current Loss on iteration 70 3794.2952\n",
            "Current Loss on iteration 71 3793.2136\n",
            "Current Loss on iteration 72 3792.215\n",
            "Current Loss on iteration 73 3791.296\n",
            "Current Loss on iteration 74 3790.4497\n",
            "Current Loss on iteration 75 3789.6687\n",
            "Current Loss on iteration 76 3788.9487\n",
            "Current Loss on iteration 77 3788.2864\n",
            "Current Loss on iteration 78 3787.6753\n",
            "Current Loss on iteration 79 3787.1128\n",
            "Current Loss on iteration 80 3786.5935\n",
            "Current Loss on iteration 81 3786.116\n",
            "Current Loss on iteration 82 3785.6729\n",
            "Current Loss on iteration 83 3785.268\n",
            "Current Loss on iteration 84 3784.8936\n",
            "Current Loss on iteration 85 3784.548\n",
            "Current Loss on iteration 86 3784.2312\n",
            "Current Loss on iteration 87 3783.9392\n",
            "Current Loss on iteration 88 3783.6672\n",
            "Current Loss on iteration 89 3783.4192\n",
            "Current Loss on iteration 90 3783.1904\n",
            "Current Loss on iteration 91 3782.9775\n",
            "Current Loss on iteration 92 3782.784\n",
            "Current Loss on iteration 93 3782.604\n",
            "Current Loss on iteration 94 3782.4375\n",
            "Current Loss on iteration 95 3782.285\n",
            "Current Loss on iteration 96 3782.1465\n",
            "Current Loss on iteration 97 3782.015\n",
            "Current Loss on iteration 98 3781.8977\n",
            "Current Loss on iteration 99 3781.7864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9smwOW-1OB7k",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9JuLI6bSOB7n",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DOL2ncA1OB7q"
      },
      "source": [
        "### Get the shapes and values of W and b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZGvtyTeuOB7r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "409f34e7-ffc7-464f-bc05-7dc366f9e605"
      },
      "source": [
        "#Check Weights and Bias\n",
        "print('Weights:\\n', w.numpy())\n",
        "print('Bias:\\n',b.numpy())"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights:\n",
            " [[2.5760543e-03]\n",
            " [2.5527244e-03]\n",
            " [2.5969257e-03]\n",
            " [3.3135368e+01]]\n",
            "Bias:\n",
            " [33.13537]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhDtOv5UOB7x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0b943b7-bc40-45b7-f690-a7ff8f50c3e0"
      },
      "source": [
        "def prediction(x, w, b):\n",
        "    \n",
        "    xw_matmul = tf.matmul(x, w)\n",
        "    y = tf.add(xw_matmul, b)\n",
        "    \n",
        "    return y\n",
        "\n",
        "prediction(train_x,w,b)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=5131535, shape=(800, 1), dtype=float32, numpy=\n",
              "array([[66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27072 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27016 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27073 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27068 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27073 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27073 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27071 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.2707  ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27073 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.270676],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ],\n",
              "       [66.27074 ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJRBuqXhOB7_"
      },
      "source": [
        "### Linear Classification using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8GoNTWXAOB8C"
      },
      "source": [
        "### Building the simple Neural Network in Keras with one neuron in the dense hidden layer.\n",
        "#### Use Mean square error as loss function and sgd as optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zpeL5rCTOB8D",
        "colab": {}
      },
      "source": [
        "#Initialize Sequential Graph (model)\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "#Add Dense layer for prediction - Keras declares weights and bias automatically\n",
        "model.add(tf.keras.layers.Dense(1, input_shape=(4,)))\n",
        "\n",
        "#Compile the model - add Loss and Gradient Descent optimizer\n",
        "model.compile(optimizer='sgd', loss='mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wt-HYFMEOB8G"
      },
      "source": [
        "### Execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66JGJt7GOB8H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e765ae0f-935b-4ebb-ad1b-fa26add2520e"
      },
      "source": [
        "model.fit(train_x, y_train, epochs=100)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "800/800 [==============================] - 0s 424us/sample - loss: 5866.3665\n",
            "Epoch 2/100\n",
            "800/800 [==============================] - 0s 29us/sample - loss: 4056.8660\n",
            "Epoch 3/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3817.3034\n",
            "Epoch 4/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3787.5310\n",
            "Epoch 5/100\n",
            "800/800 [==============================] - 0s 29us/sample - loss: 3785.6210\n",
            "Epoch 6/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3785.5769\n",
            "Epoch 7/100\n",
            "800/800 [==============================] - 0s 29us/sample - loss: 3785.0240\n",
            "Epoch 8/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3782.1934\n",
            "Epoch 9/100\n",
            "800/800 [==============================] - 0s 36us/sample - loss: 3783.6998\n",
            "Epoch 10/100\n",
            "800/800 [==============================] - 0s 36us/sample - loss: 3783.6302\n",
            "Epoch 11/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3784.6252\n",
            "Epoch 12/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3786.7925\n",
            "Epoch 13/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.7355\n",
            "Epoch 14/100\n",
            "800/800 [==============================] - 0s 37us/sample - loss: 3784.2587\n",
            "Epoch 15/100\n",
            "800/800 [==============================] - 0s 42us/sample - loss: 3784.9288\n",
            "Epoch 16/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3785.2282\n",
            "Epoch 17/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3786.8474\n",
            "Epoch 18/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.5632\n",
            "Epoch 19/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3782.5637\n",
            "Epoch 20/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3786.9729\n",
            "Epoch 21/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.0296\n",
            "Epoch 22/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3782.9927\n",
            "Epoch 23/100\n",
            "800/800 [==============================] - 0s 39us/sample - loss: 3786.2743\n",
            "Epoch 24/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3784.0797\n",
            "Epoch 25/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3785.2200\n",
            "Epoch 26/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3786.4489\n",
            "Epoch 27/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3784.0954\n",
            "Epoch 28/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3788.0782\n",
            "Epoch 29/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3787.4705\n",
            "Epoch 30/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.1580\n",
            "Epoch 31/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.1284\n",
            "Epoch 32/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3785.7553\n",
            "Epoch 33/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3783.6113\n",
            "Epoch 34/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3785.5743\n",
            "Epoch 35/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3784.6157\n",
            "Epoch 36/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3783.0909\n",
            "Epoch 37/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3785.4777\n",
            "Epoch 38/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3783.9392\n",
            "Epoch 39/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.7312\n",
            "Epoch 40/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3785.0373\n",
            "Epoch 41/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3784.4273\n",
            "Epoch 42/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3786.0947\n",
            "Epoch 43/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3783.4221\n",
            "Epoch 44/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3783.9815\n",
            "Epoch 45/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.2984\n",
            "Epoch 46/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3785.2381\n",
            "Epoch 47/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.5288\n",
            "Epoch 48/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3783.8233\n",
            "Epoch 49/100\n",
            "800/800 [==============================] - 0s 36us/sample - loss: 3785.9166\n",
            "Epoch 50/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.0679\n",
            "Epoch 51/100\n",
            "800/800 [==============================] - 0s 36us/sample - loss: 3783.8793\n",
            "Epoch 52/100\n",
            "800/800 [==============================] - 0s 27us/sample - loss: 3785.1308\n",
            "Epoch 53/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.5383\n",
            "Epoch 54/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.7755\n",
            "Epoch 55/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3783.4066\n",
            "Epoch 56/100\n",
            "800/800 [==============================] - 0s 37us/sample - loss: 3783.9517\n",
            "Epoch 57/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.6633\n",
            "Epoch 58/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3783.7469\n",
            "Epoch 59/100\n",
            "800/800 [==============================] - 0s 37us/sample - loss: 3784.4543\n",
            "Epoch 60/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3785.6775\n",
            "Epoch 61/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3782.9636\n",
            "Epoch 62/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3783.8962\n",
            "Epoch 63/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3784.4168\n",
            "Epoch 64/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3782.6123\n",
            "Epoch 65/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3782.8128\n",
            "Epoch 66/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3783.6715\n",
            "Epoch 67/100\n",
            "800/800 [==============================] - 0s 41us/sample - loss: 3784.5451\n",
            "Epoch 68/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3785.9095\n",
            "Epoch 69/100\n",
            "800/800 [==============================] - 0s 29us/sample - loss: 3785.3224\n",
            "Epoch 70/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3783.9296\n",
            "Epoch 71/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3781.5271\n",
            "Epoch 72/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3781.5775\n",
            "Epoch 73/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.4817\n",
            "Epoch 74/100\n",
            "800/800 [==============================] - 0s 42us/sample - loss: 3786.2173\n",
            "Epoch 75/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3783.0487\n",
            "Epoch 76/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3785.6639\n",
            "Epoch 77/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3785.8311\n",
            "Epoch 78/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3785.4966\n",
            "Epoch 79/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3785.8300\n",
            "Epoch 80/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3785.1235\n",
            "Epoch 81/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3784.8147\n",
            "Epoch 82/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.0172\n",
            "Epoch 83/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3784.5529\n",
            "Epoch 84/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3783.6025\n",
            "Epoch 85/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.2271\n",
            "Epoch 86/100\n",
            "800/800 [==============================] - 0s 35us/sample - loss: 3784.6345\n",
            "Epoch 87/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3783.9005\n",
            "Epoch 88/100\n",
            "800/800 [==============================] - 0s 38us/sample - loss: 3784.4966\n",
            "Epoch 89/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3783.7742\n",
            "Epoch 90/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3785.3458\n",
            "Epoch 91/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3782.6895\n",
            "Epoch 92/100\n",
            "800/800 [==============================] - 0s 29us/sample - loss: 3784.7258\n",
            "Epoch 93/100\n",
            "800/800 [==============================] - 0s 34us/sample - loss: 3783.4845\n",
            "Epoch 94/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3785.3356\n",
            "Epoch 95/100\n",
            "800/800 [==============================] - 0s 37us/sample - loss: 3783.2461\n",
            "Epoch 96/100\n",
            "800/800 [==============================] - 0s 30us/sample - loss: 3784.5801\n",
            "Epoch 97/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3784.9508\n",
            "Epoch 98/100\n",
            "800/800 [==============================] - 0s 31us/sample - loss: 3786.2675\n",
            "Epoch 99/100\n",
            "800/800 [==============================] - 0s 32us/sample - loss: 3785.2454\n",
            "Epoch 100/100\n",
            "800/800 [==============================] - 0s 33us/sample - loss: 3784.5903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c79a9aba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhXzJNN0cYBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "44f2e395-238e-437e-d105-b8ef0a5562a4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 5\n",
            "Trainable params: 5\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCQwzc6ZcZmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "f70ae4d7-22ab-43a2-ed72-2aa1dd69b9fa"
      },
      "source": [
        "model.get_weights()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.25881302],\n",
              "        [ 0.04899972],\n",
              "        [ 0.07619543],\n",
              "        [33.563053  ]], dtype=float32), array([34.632206], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhtWs4oLLO6M",
        "colab_type": "text"
      },
      "source": [
        "### Classification using Keras "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE5pP3ciLO6O",
        "colab_type": "text"
      },
      "source": [
        "### Load the given Iris data using pandas (Iris.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZY39vBbLO6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = pd.read_csv('Iris-2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ary-A7jDLO6P",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the data into feature set and target set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ogmWy-mLO6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d543172c-fb53-48d3-b093-57264052d8d9"
      },
      "source": [
        "iris.columns"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm',\n",
              "       'Species'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xNJ5GnTdfM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=iris.drop(columns=['Id','Species'])\n",
        "Y=iris.Species"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpwjgs_tdfXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2l6k6lQdfiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train,X_test,y_train,y_test= train_test_split(X,Y,test_size =0.3,random_state = 123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR5QF_pdLO6S",
        "colab_type": "text"
      },
      "source": [
        "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNRZYAqgfQ9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "f2474331-4620-4698-f1ec-a0a954679b94"
      },
      "source": [
        "Y.value_counts()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Iris-setosa        50\n",
              "Iris-virginica     50\n",
              "Iris-versicolor    50\n",
              "Name: Species, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLZr3AKnLO6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = pd.get_dummies(Y)\n",
        "#testX = tf.keras.utils.to_categorical(np.array(X_test), num_classes=4)\n",
        "#trainY = tf.keras.utils.to_categorical(y_train, num_classes=4)\n",
        "#testY = tf.keras.utils.to_categorical(y_test, num_classes=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4bLlfQmLO6V",
        "colab_type": "text"
      },
      "source": [
        "### Divide the dataset into Training and test (70:30)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Ig4ulWLO6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test= train_test_split(X,Y,test_size =0.3,random_state = 123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GlbpGYELO6Z",
        "colab_type": "text"
      },
      "source": [
        "### Model\n",
        "Build the model with following layers: <br>\n",
        "1. First dense layer with 10 neurons with input shape 4 (according to the feature set) <br>\n",
        "2. Second Dense layer with 8 neurons <br>\n",
        "3. Output layer with 3 neurons with softmax activation (output layer, 3 neurons as we have 3 classes) <br>\n",
        "4. Use SGD and categorical_crossentropy loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsVVUTSyLO6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "#model.add(tf.keras.layers.Re,input_shape=(4,)))\n",
        "\n",
        "#Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization(input_shape=(4,)))\n",
        "\n",
        "#Add 1st hidden layer\n",
        "model.add(tf.keras.layers.Dense(10, activation='sigmoid'))\n",
        "#Add 2nd hidden layer\n",
        "model.add(tf.keras.layers.Dense(8, activation='sigmoid'))\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "#Comile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgHR-6MEkD0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70LKIEm3LO6b",
        "colab_type": "text"
      },
      "source": [
        "### Fitting the model and predicting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSJ9pa4ULO6d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf06d935-482a-4920-a7ab-daf3603a3e80"
      },
      "source": [
        "model.fit(X_train, y_train, \n",
        "          validation_data=(X_test, y_test), \n",
        "          epochs=80,\n",
        "          batch_size=10)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 45 samples\n",
            "Epoch 1/80\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 1.0521 - acc: 0.4000 - val_loss: 1.0644 - val_acc: 0.2667\n",
            "Epoch 2/80\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 1.0458 - acc: 0.4381 - val_loss: 1.0622 - val_acc: 0.2667\n",
            "Epoch 3/80\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 1.0453 - acc: 0.4095 - val_loss: 1.0610 - val_acc: 0.2667\n",
            "Epoch 4/80\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 1.0430 - acc: 0.4286 - val_loss: 1.0601 - val_acc: 0.2667\n",
            "Epoch 5/80\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 1.0472 - acc: 0.4476 - val_loss: 1.0583 - val_acc: 0.2667\n",
            "Epoch 6/80\n",
            "105/105 [==============================] - 0s 208us/sample - loss: 1.0421 - acc: 0.4381 - val_loss: 1.0564 - val_acc: 0.2667\n",
            "Epoch 7/80\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 1.0409 - acc: 0.4286 - val_loss: 1.0554 - val_acc: 0.2667\n",
            "Epoch 8/80\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 1.0439 - acc: 0.4095 - val_loss: 1.0545 - val_acc: 0.2667\n",
            "Epoch 9/80\n",
            "105/105 [==============================] - 0s 217us/sample - loss: 1.0384 - acc: 0.4286 - val_loss: 1.0524 - val_acc: 0.2889\n",
            "Epoch 10/80\n",
            "105/105 [==============================] - 0s 206us/sample - loss: 1.0386 - acc: 0.4286 - val_loss: 1.0504 - val_acc: 0.2889\n",
            "Epoch 11/80\n",
            "105/105 [==============================] - 0s 210us/sample - loss: 1.0396 - acc: 0.4286 - val_loss: 1.0485 - val_acc: 0.2889\n",
            "Epoch 12/80\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 1.0401 - acc: 0.4476 - val_loss: 1.0465 - val_acc: 0.2889\n",
            "Epoch 13/80\n",
            "105/105 [==============================] - 0s 220us/sample - loss: 1.0377 - acc: 0.4571 - val_loss: 1.0472 - val_acc: 0.2889\n",
            "Epoch 14/80\n",
            "105/105 [==============================] - 0s 209us/sample - loss: 1.0390 - acc: 0.4571 - val_loss: 1.0448 - val_acc: 0.2889\n",
            "Epoch 15/80\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 1.0399 - acc: 0.4476 - val_loss: 1.0441 - val_acc: 0.2889\n",
            "Epoch 16/80\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 1.0322 - acc: 0.4476 - val_loss: 1.0428 - val_acc: 0.2889\n",
            "Epoch 17/80\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 1.0332 - acc: 0.4381 - val_loss: 1.0416 - val_acc: 0.2889\n",
            "Epoch 18/80\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 1.0351 - acc: 0.4190 - val_loss: 1.0408 - val_acc: 0.2889\n",
            "Epoch 19/80\n",
            "105/105 [==============================] - 0s 238us/sample - loss: 1.0318 - acc: 0.4857 - val_loss: 1.0383 - val_acc: 0.2889\n",
            "Epoch 20/80\n",
            "105/105 [==============================] - 0s 205us/sample - loss: 1.0265 - acc: 0.4571 - val_loss: 1.0375 - val_acc: 0.2889\n",
            "Epoch 21/80\n",
            "105/105 [==============================] - 0s 228us/sample - loss: 1.0306 - acc: 0.4667 - val_loss: 1.0376 - val_acc: 0.2889\n",
            "Epoch 22/80\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 1.0260 - acc: 0.4857 - val_loss: 1.0364 - val_acc: 0.2889\n",
            "Epoch 23/80\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 1.0275 - acc: 0.4476 - val_loss: 1.0349 - val_acc: 0.2889\n",
            "Epoch 24/80\n",
            "105/105 [==============================] - 0s 196us/sample - loss: 1.0242 - acc: 0.4571 - val_loss: 1.0331 - val_acc: 0.2889\n",
            "Epoch 25/80\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 1.0211 - acc: 0.4476 - val_loss: 1.0304 - val_acc: 0.2889\n",
            "Epoch 26/80\n",
            "105/105 [==============================] - 0s 208us/sample - loss: 1.0290 - acc: 0.4286 - val_loss: 1.0274 - val_acc: 0.3111\n",
            "Epoch 27/80\n",
            "105/105 [==============================] - 0s 213us/sample - loss: 1.0192 - acc: 0.4476 - val_loss: 1.0252 - val_acc: 0.3333\n",
            "Epoch 28/80\n",
            "105/105 [==============================] - 0s 220us/sample - loss: 1.0213 - acc: 0.5238 - val_loss: 1.0229 - val_acc: 0.3556\n",
            "Epoch 29/80\n",
            "105/105 [==============================] - 0s 211us/sample - loss: 1.0189 - acc: 0.4857 - val_loss: 1.0207 - val_acc: 0.3778\n",
            "Epoch 30/80\n",
            "105/105 [==============================] - 0s 205us/sample - loss: 1.0226 - acc: 0.4762 - val_loss: 1.0199 - val_acc: 0.3556\n",
            "Epoch 31/80\n",
            "105/105 [==============================] - 0s 202us/sample - loss: 1.0175 - acc: 0.4952 - val_loss: 1.0165 - val_acc: 0.4222\n",
            "Epoch 32/80\n",
            "105/105 [==============================] - 0s 226us/sample - loss: 1.0194 - acc: 0.5143 - val_loss: 1.0147 - val_acc: 0.4444\n",
            "Epoch 33/80\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 1.0099 - acc: 0.4952 - val_loss: 1.0140 - val_acc: 0.4222\n",
            "Epoch 34/80\n",
            "105/105 [==============================] - 0s 204us/sample - loss: 1.0178 - acc: 0.5524 - val_loss: 1.0155 - val_acc: 0.3556\n",
            "Epoch 35/80\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 1.0115 - acc: 0.4857 - val_loss: 1.0136 - val_acc: 0.3556\n",
            "Epoch 36/80\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 1.0195 - acc: 0.4857 - val_loss: 1.0115 - val_acc: 0.3556\n",
            "Epoch 37/80\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 1.0085 - acc: 0.4476 - val_loss: 1.0088 - val_acc: 0.3556\n",
            "Epoch 38/80\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 1.0092 - acc: 0.5143 - val_loss: 1.0052 - val_acc: 0.4222\n",
            "Epoch 39/80\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 1.0078 - acc: 0.5143 - val_loss: 1.0028 - val_acc: 0.4444\n",
            "Epoch 40/80\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 1.0075 - acc: 0.4857 - val_loss: 1.0016 - val_acc: 0.4444\n",
            "Epoch 41/80\n",
            "105/105 [==============================] - 0s 211us/sample - loss: 1.0020 - acc: 0.4476 - val_loss: 0.9993 - val_acc: 0.4667\n",
            "Epoch 42/80\n",
            "105/105 [==============================] - 0s 207us/sample - loss: 1.0032 - acc: 0.5524 - val_loss: 0.9980 - val_acc: 0.4667\n",
            "Epoch 43/80\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 1.0018 - acc: 0.5143 - val_loss: 0.9955 - val_acc: 0.4889\n",
            "Epoch 44/80\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.9981 - acc: 0.5619 - val_loss: 0.9931 - val_acc: 0.4889\n",
            "Epoch 45/80\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.9970 - acc: 0.5524 - val_loss: 0.9908 - val_acc: 0.5111\n",
            "Epoch 46/80\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 1.0003 - acc: 0.5714 - val_loss: 0.9896 - val_acc: 0.5111\n",
            "Epoch 47/80\n",
            "105/105 [==============================] - 0s 234us/sample - loss: 0.9969 - acc: 0.5238 - val_loss: 0.9872 - val_acc: 0.5111\n",
            "Epoch 48/80\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.9899 - acc: 0.5619 - val_loss: 0.9866 - val_acc: 0.5111\n",
            "Epoch 49/80\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.9980 - acc: 0.5619 - val_loss: 0.9836 - val_acc: 0.4889\n",
            "Epoch 50/80\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.9958 - acc: 0.5524 - val_loss: 0.9810 - val_acc: 0.5111\n",
            "Epoch 51/80\n",
            "105/105 [==============================] - 0s 208us/sample - loss: 0.9944 - acc: 0.5238 - val_loss: 0.9788 - val_acc: 0.5333\n",
            "Epoch 52/80\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.9840 - acc: 0.6000 - val_loss: 0.9780 - val_acc: 0.5111\n",
            "Epoch 53/80\n",
            "105/105 [==============================] - 0s 213us/sample - loss: 0.9825 - acc: 0.5905 - val_loss: 0.9768 - val_acc: 0.5111\n",
            "Epoch 54/80\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 0.9798 - acc: 0.6095 - val_loss: 0.9754 - val_acc: 0.4889\n",
            "Epoch 55/80\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.9787 - acc: 0.5429 - val_loss: 0.9709 - val_acc: 0.5556\n",
            "Epoch 56/80\n",
            "105/105 [==============================] - 0s 215us/sample - loss: 0.9782 - acc: 0.6095 - val_loss: 0.9676 - val_acc: 0.5778\n",
            "Epoch 57/80\n",
            "105/105 [==============================] - 0s 211us/sample - loss: 0.9800 - acc: 0.6476 - val_loss: 0.9660 - val_acc: 0.5778\n",
            "Epoch 58/80\n",
            "105/105 [==============================] - 0s 228us/sample - loss: 0.9730 - acc: 0.6190 - val_loss: 0.9640 - val_acc: 0.5778\n",
            "Epoch 59/80\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 0.9688 - acc: 0.6476 - val_loss: 0.9613 - val_acc: 0.5778\n",
            "Epoch 60/80\n",
            "105/105 [==============================] - 0s 215us/sample - loss: 0.9732 - acc: 0.6095 - val_loss: 0.9596 - val_acc: 0.5778\n",
            "Epoch 61/80\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.9674 - acc: 0.6095 - val_loss: 0.9574 - val_acc: 0.5778\n",
            "Epoch 62/80\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 0.9698 - acc: 0.5714 - val_loss: 0.9565 - val_acc: 0.5778\n",
            "Epoch 63/80\n",
            "105/105 [==============================] - 0s 218us/sample - loss: 0.9740 - acc: 0.5619 - val_loss: 0.9549 - val_acc: 0.5778\n",
            "Epoch 64/80\n",
            "105/105 [==============================] - 0s 209us/sample - loss: 0.9621 - acc: 0.6667 - val_loss: 0.9521 - val_acc: 0.5778\n",
            "Epoch 65/80\n",
            "105/105 [==============================] - 0s 214us/sample - loss: 0.9563 - acc: 0.6762 - val_loss: 0.9491 - val_acc: 0.5778\n",
            "Epoch 66/80\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.9741 - acc: 0.6000 - val_loss: 0.9468 - val_acc: 0.5778\n",
            "Epoch 67/80\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.9639 - acc: 0.5810 - val_loss: 0.9448 - val_acc: 0.5778\n",
            "Epoch 68/80\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 0.9614 - acc: 0.5810 - val_loss: 0.9402 - val_acc: 0.6000\n",
            "Epoch 69/80\n",
            "105/105 [==============================] - 0s 215us/sample - loss: 0.9515 - acc: 0.6952 - val_loss: 0.9386 - val_acc: 0.6000\n",
            "Epoch 70/80\n",
            "105/105 [==============================] - 0s 212us/sample - loss: 0.9541 - acc: 0.6095 - val_loss: 0.9337 - val_acc: 0.6444\n",
            "Epoch 71/80\n",
            "105/105 [==============================] - 0s 213us/sample - loss: 0.9519 - acc: 0.6857 - val_loss: 0.9321 - val_acc: 0.6444\n",
            "Epoch 72/80\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.9495 - acc: 0.6762 - val_loss: 0.9302 - val_acc: 0.6444\n",
            "Epoch 73/80\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 0.9502 - acc: 0.6667 - val_loss: 0.9257 - val_acc: 0.6889\n",
            "Epoch 74/80\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 0.9405 - acc: 0.6667 - val_loss: 0.9215 - val_acc: 0.7111\n",
            "Epoch 75/80\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 0.9416 - acc: 0.6667 - val_loss: 0.9166 - val_acc: 0.7556\n",
            "Epoch 76/80\n",
            "105/105 [==============================] - 0s 215us/sample - loss: 0.9375 - acc: 0.7143 - val_loss: 0.9143 - val_acc: 0.7556\n",
            "Epoch 77/80\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.9442 - acc: 0.7048 - val_loss: 0.9100 - val_acc: 0.7778\n",
            "Epoch 78/80\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 0.9298 - acc: 0.7714 - val_loss: 0.9066 - val_acc: 0.8222\n",
            "Epoch 79/80\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.9374 - acc: 0.6952 - val_loss: 0.9047 - val_acc: 0.8000\n",
            "Epoch 80/80\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.9304 - acc: 0.7429 - val_loss: 0.9013 - val_acc: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c772f84a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMl7tQ3pLO6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWaHOWlLj-0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10tHGSsGLO6h",
        "colab_type": "text"
      },
      "source": [
        "### Report Accuracy of the predicted values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4vJbpVNLO6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "5079ebcf-b393-41bc-ff2f-af0a719277ab"
      },
      "source": [
        "pred = model.predict(X_test[:])\n",
        "pred"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.28994578, 0.39147985, 0.3185744 ],\n",
              "       [0.29842725, 0.37391034, 0.32766235],\n",
              "       [0.29632148, 0.37498888, 0.32868966],\n",
              "       [0.29410332, 0.38428614, 0.32161048],\n",
              "       [0.2845145 , 0.4137634 , 0.30172214],\n",
              "       [0.29680473, 0.37659574, 0.32659954],\n",
              "       [0.2860022 , 0.40128824, 0.3127096 ],\n",
              "       [0.2864384 , 0.40860346, 0.30495805],\n",
              "       [0.28206563, 0.41515535, 0.30277905],\n",
              "       [0.28935093, 0.39093882, 0.31971028],\n",
              "       [0.2986132 , 0.37529546, 0.32609132],\n",
              "       [0.30007333, 0.38145933, 0.31846738],\n",
              "       [0.28986698, 0.38993487, 0.32019815],\n",
              "       [0.29841793, 0.37333077, 0.32825136],\n",
              "       [0.29897013, 0.3741832 , 0.32684666],\n",
              "       [0.2994279 , 0.36857226, 0.3319999 ],\n",
              "       [0.29003954, 0.40583637, 0.30412406],\n",
              "       [0.2824863 , 0.41584933, 0.30166444],\n",
              "       [0.29448703, 0.383553  , 0.32195994],\n",
              "       [0.28326896, 0.415369  , 0.30136204],\n",
              "       [0.28893968, 0.40689757, 0.3041627 ],\n",
              "       [0.29436412, 0.38260844, 0.32302746],\n",
              "       [0.291065  , 0.39935732, 0.30957767],\n",
              "       [0.2970291 , 0.37633097, 0.3266399 ],\n",
              "       [0.2957644 , 0.3912184 , 0.31301722],\n",
              "       [0.2897237 , 0.4050516 , 0.30522472],\n",
              "       [0.28957516, 0.4061837 , 0.30424115],\n",
              "       [0.2947533 , 0.37744385, 0.32780284],\n",
              "       [0.3098568 , 0.3520407 , 0.33810255],\n",
              "       [0.29231748, 0.39922807, 0.3084544 ],\n",
              "       [0.29663756, 0.37837264, 0.32498974],\n",
              "       [0.28673303, 0.3957525 , 0.31751445],\n",
              "       [0.28698704, 0.4077726 , 0.30524036],\n",
              "       [0.29064137, 0.40305915, 0.3062995 ],\n",
              "       [0.28321695, 0.40684092, 0.30994216],\n",
              "       [0.28231153, 0.4058519 , 0.31183666],\n",
              "       [0.30933195, 0.35607433, 0.33459368],\n",
              "       [0.29750437, 0.38785368, 0.3146419 ],\n",
              "       [0.29648748, 0.39136115, 0.31215137],\n",
              "       [0.2892151 , 0.39562508, 0.3151599 ],\n",
              "       [0.27985197, 0.40981266, 0.31033534],\n",
              "       [0.28523752, 0.41400585, 0.3007566 ],\n",
              "       [0.30211076, 0.36436778, 0.3335215 ],\n",
              "       [0.2977394 , 0.37518805, 0.32707262],\n",
              "       [0.2937771 , 0.37983453, 0.3263884 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcUYGgREkOFP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a879878a-d7cf-4cf3-a974-596ff597916a"
      },
      "source": [
        "pred[3]"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.29410332, 0.38428614, 0.32161048], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DxJohq_kWOD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "1cf1e2a8-88f3-401f-ec8d-d20606492226"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_6 (Batch (None, 4)                 16        \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                50        \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 8)                 88        \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 27        \n",
            "=================================================================\n",
            "Total params: 181\n",
            "Trainable params: 173\n",
            "Non-trainable params: 8\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvucz4eik3xo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOarD2w_lDvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.sc"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}